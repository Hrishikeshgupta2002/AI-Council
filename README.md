# Agentic Council System

**A fully free, open-source AI council system** built with **LangChain** and **Ollama**. This beautiful CLI-based multi-agent system uses 4 specialized AI agents to analyze problems through parallel responses, natural group chat discussions, and weighted synthesis - **completely free, no API costs, runs entirely locally**.

![Agentic Council System Demo](Untitled%20design.gif)

## ğŸ†“ 100% Free & Open Source

- âœ… **No API costs** - Runs entirely on your local machine with Ollama
- âœ… **No cloud dependencies** - All processing happens locally
- âœ… **Open source** - MIT licensed, fully customizable
- âœ… **Privacy-first** - Your data never leaves your machine
- âœ… **Powered by LangChain** - Industry-standard AI framework
- âœ… **Powered by Ollama** - Free, local LLM inference

## ğŸ¯ Features

- ğŸ†“ **100% Free** - No API costs, runs entirely locally with Ollama
- ğŸ¨ **Beautiful CLI** with rich formatting and colors
- ğŸ’¬ **Natural group chat** conversations between agents
- ğŸ·ï¸ **Tag agents** with @mentions for focused debates
- âš–ï¸ **Weighted decision model** for final recommendations
- ğŸ” **Comprehensive synthesis** analysis
- ğŸš€ **Parallel agent processing** for speed
- ğŸ“Š **Structured output** with agreements, conflicts, and blind spots
- ğŸ”’ **Privacy-first** - All data stays on your machine
- ğŸ› ï¸ **Built with LangChain** - Industry-standard AI framework

## ğŸ—ï¸ Architecture

The system consists of 4 specialized agents, each with a distinct persona and thinking layer:

1. **Elon** (Visionary) - First-principles thinking, innovation, bold direction
2. **Sam** (Strategist) - Business model, market realities, scalability
3. **Sheryl** (Operator) - Practical execution, system design, reliability
4. **Ray** (Risk Analyst) - Red-team, failure modes, blind spots

## ğŸ“¦ Installation

### Prerequisites

1. **Python 3.8+** installed
2. **Ollama** (free, local LLM runtime) - Download from [ollama.ai](https://ollama.ai)
   ```bash
   # Install Ollama, then start it:
   ollama serve
   ```

3. **Pull required models (all free):**
   ```bash
   # These models run entirely locally - no API costs!
   ollama pull gpt-oss:120b-cloud
   ollama pull glm-4.6:cloud
   ollama pull kimi-k2-thinking:cloud
   ollama pull deepseek-v3.1:671b-cloud
   ```
   
   **Note:** Model downloads are free. They run locally on your machine.

### Install from Source

```bash
# Clone the repository
git clone https://github.com/Hrishikeshgupta2002/AI-Council.git
cd AI-Council

# Install dependencies
pip install -r requirements.txt

# Or install as a package
pip install -e .
```

## ğŸš€ Usage

### Basic Usage

```bash
python main.py
```

Enter your problem statement when prompted, and the council will analyze it.

### Command Line Arguments

```bash
# Direct input
python main.py "Your problem statement here"

# Piped input
echo "Your problem statement" | python main.py
```

### Interactive Features

- **Normal message**: Type your message, all agents respond
- **Tag agents**: Use `@Elon @Sam debate this topic` to start a focused debate
- **Continue conversation**: Press Enter to continue the general conversation
- **Exit**: Type `exit`, `quit`, or `q` to end

### Example

```bash
$ python main.py
Enter your problem statement: Should we build this feature?

# Agents respond...

> @Elon @Sam debate the technical approach
# Tagged agents have a focused 2-3 exchange debate

> # Press Enter for another round
# Agents can choose to respond or skip
```

## âš™ï¸ Configuration

Set environment variables to customize behavior:

```bash
export OLLAMA_BASE_URL="http://localhost:11434"  # Ollama server URL
export USE_WEIGHTED_MODEL="true"                  # Use weighted model (default: true)
export AGENT_TIMEOUT="60"                         # Agent response timeout in seconds
export MAX_WORKERS="4"                            # Max parallel workers
export DEBUG="true"                                # Show detailed error traces
```

## ğŸ“ Project Structure

```
agentic-council/
â”œâ”€â”€ agents/              # Agent implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ visionary.py
â”‚   â”œâ”€â”€ strategist.py
â”‚   â”œâ”€â”€ operator.py
â”‚   â””â”€â”€ risk_analyst.py
â”œâ”€â”€ council.py           # Council orchestrator
â”œâ”€â”€ synthesis.py         # Synthesis agent
â”œâ”€â”€ state.py             # State management
â”œâ”€â”€ config.py            # Configuration module
â”œâ”€â”€ main.py              # CLI entry point
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ setup.py            # Package setup
â”œâ”€â”€ pyproject.toml      # Modern Python packaging
â”œâ”€â”€ LICENSE             # MIT License
â””â”€â”€ README.md           # This file
```

## ğŸ§© Module Overview

- **`council.py`**: Main orchestrator managing agent interactions
- **`agents/`**: Individual agent implementations with distinct personas
- **`synthesis.py`**: Meta-agent that synthesizes all responses
- **`state.py`**: State management for conversation tracking
- **`config.py`**: Centralized configuration management
- **`main.py`**: CLI interface with rich formatting

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ’¡ Why This is Free

This project is **completely free** because:

- **Ollama** provides free, local LLM inference - no API costs
- **LangChain** is an open-source framework - no licensing fees
- **All processing is local** - no cloud services or subscriptions needed
- **Open source** - MIT licensed, you own the code

Unlike commercial AI services that charge per API call, this system runs entirely on your hardware using free, open-source models.

## ğŸ™ Acknowledgments

- Built with [LangChain](https://github.com/langchain-ai/langchain) - Open-source AI framework
- Powered by [Ollama](https://ollama.ai/) - Free, local LLM inference
- Beautiful CLI with [Rich](https://github.com/Textualize/rich) - Terminal formatting library
- All models are free and run locally - no API costs ever!
